{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CS 5785 - Final\n",
    "## Applied Machine Learning\n",
    "\n",
    "Eric Nguyen *(en274)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This program was used for a Kaggle challenge to create the highest accurate match of a text description and a corresponding photo.\n",
    "\n",
    "# This program uses word2vec to match text descriptions to the closest photo matching that description.  Both the training and test photos are tagged with keywords and descriptions.  \n",
    "\n",
    "# We are given 10,000 training photos, each with 5 sentences describing the photo.  We are also given 2,000 test photos, also with 5 sentences describing the photo.\n",
    "\n",
    "# word2vec is a neural network model from Google that maps words to their most common surrounding words, allowing you incorporate the context, or order of words, in your prediction.  We will be using the gensim library to access the word2vec model, and train it with our own data.\n",
    "\n",
    "# From a high level, word2vec works by taking a large corpus of sentences, and understands the context of words in sentences.  It maps a word to other commonly associated words.  For example, \"dog\" might commonly occur with \"pet\", \"animal\" or even \"cat\".  \n",
    "\n",
    "# After training with a corpus, a high dimensional space is created.  This allows you to take a word vector and project it into this high dimensional space.  What you might observe is that words that commonly occur together will project to the same region or cluster near each other in this \"space\".\n",
    "\n",
    "# For our purposes, we take a given test sentence, averaged all the word vectors, and then found its nearest neighbor word2vec based on cosine similarity distance, in the training data.  We then mapped the nearest 20 neighbor descriptions in the test data and used their photo labels as the predictions to submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import setup\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import Word2Vec\n",
    "from IPython.display import display, Markdown\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors as KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN = 0  # this index indicates which part of data is for training\n",
    "TEST = 1  # this index indicates which part of data is for test\n",
    "N_TRAIN = 10000  # number of training images\n",
    "N_TEST = 2000  #  number of test data\n",
    "N_DESCRIPTIONS = 10000  # number of descriptions\n",
    "\n",
    "# Placeholders for gathering the descriptions from the photos (Pandas data frames)\n",
    "descriptions_train = []\n",
    "descriptions_test = []\n",
    "\n",
    "# get the image features from the dataset\n",
    "features_train = setup.get_features(False, TRAIN)\n",
    "features_test = setup.get_features(False, TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using our own utility module, we call these functions to pull from the\n",
    "# descriptions from training and test data\n",
    "descriptions_train = setup.get_descriptions(N_TRAIN,TRAIN)\n",
    "descriptions_test = setup.get_descriptions(N_TEST,TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(descriptions, dataset):\n",
    "    \"\"\"\n",
    "    Given a Pandas DataFrame of training or testing descriptions, \n",
    "    this function:\n",
    "    1. Combines the five sentence descriptions into list of words\n",
    "    2. Preprocesses the combined description by lowercasing words,\n",
    "       removing punctuation, removing stopwords, and stemming words.\n",
    "    3. Returns a Pandas DataFrame with one column consisting of a list\n",
    "    of preprocessed words.\n",
    "    \n",
    "    :param descriptions: Pandas dataframe\n",
    "    :param dataset: raw dataset, either training or testing\n",
    "    \n",
    "    :return: result, Pandas dataframe\n",
    "    \n",
    "    \"\"\"    \n",
    "    preproc_words = []  # placeholder for return of preprocessed words\n",
    "\n",
    "    # iterate through every description\n",
    "    for index, row in descriptions.iterrows():\n",
    "        \n",
    "        for sentence in row.tolist():\n",
    "            sentence = sentence.split()  # separate by white spaces\n",
    "            \n",
    "            # Lowercase each word\n",
    "            sentence = [word.lower() for word in sentence]\n",
    "\n",
    "            # Remove punctuation from words\n",
    "            sentence = [word.translate(str.maketrans(dict.fromkeys(string.punctuation))) for word in sentence]\n",
    "\n",
    "            # Remove stopwords\n",
    "            sentence = [word for word in sentence if word not in stopwords.words('english')]\n",
    "\n",
    "            # Stem words\n",
    "            ps = PorterStemmer()\n",
    "            sentence = [ps.stem(word) for word in sentence]\n",
    "\n",
    "            # removes empty elements\n",
    "            sentence = list(filter(None, sentence))\n",
    "            \n",
    "            preproc_words.append(sentence)  # include preprocessed sentence\n",
    "            \n",
    "    # save into a csv file to reduce having to repeat this step when \n",
    "    # testing\n",
    "    output = pd.DataFrame(preproc_words)\n",
    "    output.to_csv(\"preprocess_description_EN\" + str(dataset) + \".csv\")\n",
    "    return preproc_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_preprocess(dataset):\n",
    "    \"\"\"\n",
    "    Given the type of dataset, TRAIN or TEST,\n",
    "    read the preprocessed descriptions from a file\n",
    "    and return them as a Pandas DataFrame.\n",
    "    \n",
    "    :param: dataset, the raw text data\n",
    "    :return: preprocess, Pandas dataframe of preprocessed words\n",
    "    \n",
    "    \"\"\"\n",
    "    preprocess = pd.read_csv(\"preprocess_description_EN\" + str(dataset) + \".csv\", index_col=0)\n",
    "    return preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess both the training and testing (data) sentence descriptions\n",
    "pre_pro_train = preprocess(descriptions_train,TRAIN)\n",
    "pre_pro_test = preprocess(descriptions_test,TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train word2vec with our own training data corpus\n",
    "# using our preprocessed sentences\n",
    "model = gensim.models.Word2Vec(pre_pro_train, size=500,window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_avg_word2vec(sentence):\n",
    "    '''\n",
    "    This function creates a word2vec for every word in a sentence.\n",
    "    It then averages over all the words in a sentence to create a\n",
    "    word2vec for that sentence.\n",
    "    \n",
    "    :param: sentence, list, description of a photo\n",
    "    \n",
    "    :return: avg_w2v, np.array, description vector projected into the\n",
    "    pretrained word2vec space.\n",
    "    \n",
    "    '''\n",
    "    # for testing purposes\n",
    "    if sentence == \"\" or sentence == \" \":\n",
    "        print('found error')\n",
    "    \n",
    "    count = 0  # track number of words\n",
    "    avg_w2v = []  # initialize empty array\n",
    "\n",
    "    # iterate through all words and sentences\n",
    "    for word in sentence:\n",
    "        if word in model:  # only if the word is in the model\n",
    "            count += 1   # increment count of words\n",
    "            single_w2v = model.wv[word]    # generate word2vec\n",
    "            single_w2v = np.array(single_w2v)  # convert to np array\n",
    "            avg_w2v.append(single_w2v)  # append the sentence w2v\n",
    "    \n",
    "    avg_w2v = np.array(avg_w2v)  # convert to np array\n",
    "    avg_w2v = np.mean(avg_w2v,axis=0)  # average over each word\n",
    "    return avg_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_list(sentence_list):\n",
    "    '''\n",
    "    This function takes in a list of sentences, and outputs \n",
    "    an average w2v for each sentence in a list.  It uses \n",
    "    get_avg_word2vec() as a helper.\n",
    "    \n",
    "    :param: sentence_list, list of sentence descriptions\n",
    "    \n",
    "    :return: w2v_list, list, word2vec for every sentence in a list\n",
    "    \n",
    "    '''\n",
    "    w2v_list = []  # declare an output list\n",
    "    \n",
    "    # iterate through every sentence list\n",
    "    for sentence in sentence_list:\n",
    "        if sentence == []:\n",
    "            sentence_w2v = get_avg_word2vec('car')  # for testing purposes\n",
    "        else:\n",
    "            sentence_w2v = get_avg_word2vec(sentence)  # get word2vec for sentence\n",
    "        w2v_list.append(sentence_w2v)  # append to output var\n",
    "    return w2v_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_w2v_list_by_description(sentence_list):\n",
    "    \n",
    "    '''\n",
    "    This function aggregates a word2vec by description, which has\n",
    "    5 sentences.  This is purely for organizing word2vecs by photo\n",
    "    and uses other helper functions to create the word2vecs, and is\n",
    "    made simple since the sentence_list is in order, ie, the first 5 \n",
    "    sentences belong to photo 1, and the next 5 to photo 2, and so on.\n",
    "    This was necessary for testing.\n",
    "    \n",
    "    :param: sentence_list, list, list of all sentences in the training data,\n",
    "    size 50,000\n",
    "    \n",
    "    :return: grouped_w2v, np array of word2vecs for a description, \n",
    "    size 10,000 (number of images) by 5 (sentences per image)\n",
    "    \n",
    "    '''\n",
    "    sentence_w2v_list = []  # initialize empty array\n",
    "    grouped_w2v = []  # initialize output np array\n",
    "    \n",
    "    # go through entire sentence list (all 50,000 sentences)\n",
    "    for sentence in sentence_list:\n",
    "        sentence_w2v = get_avg_word2vec(sentence)  # get w2v for sentence\n",
    "        sentence_w2v = np.array(sentence_w2v)  # convert to np array\n",
    "        sentence_w2v_list.append(sentence_w2v)  # add to a list\n",
    "        \n",
    "    # iterate through all 10000 photos and assign the 5 word2vecs per photo\n",
    "    for i in range(0,len(sentence_w2v_list),5):       \n",
    "        temp = sentence_w2v_list[i:i+5]  # assigning the next 5 word2vecs\n",
    "        temp = np.array(temp)\n",
    "        avg_w2v_sentence = np.mean(temp,axis=0)  # average them\n",
    "        grouped_w2v.append(avg_w2v_sentence)  # append the 5 sentences to grouped output\n",
    "    return grouped_w2v  # returned the aggregated w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -------------------   # testing starts here. ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get preprocessed word2vecs from training data descriptions, use as\n",
    "# inputs to our classifier\n",
    "train_x_vals = get_w2v_list(pre_pro_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create label vectors for training data.  The photo label\n",
    "# is in the features file in the training data, and has the form\n",
    "# images_train/5373.jpg, where the 4 digit number varies. Each label\n",
    "# is in order, so we know what descriptions match to which labels\n",
    "# (supervised learning)\n",
    "features_train = setup.get_features(False,TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_labels(n_elements):\n",
    "    '''\n",
    "    This function creates a label for each description by pulling the \n",
    "    the photo label from the image features file and matching\n",
    "    them to the descriptions.  (We do not know which labels correspend\n",
    "    to which description in the test data, this is what we are predicting.)\n",
    "    \n",
    "    :param: n_elements, list, a photo from the image features file\n",
    "    \n",
    "    :return: y_vals, list, labels for each photo description\n",
    "    '''\n",
    "    y_vals = []\n",
    "    for i in range(n_elements):\n",
    "        for j in range(5):\n",
    "            # assign the photo label to 5 descriptions at a time\n",
    "            y_vals.append(pic_index)\n",
    "            pic_index = features_train.index[i]  \n",
    "    return y_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get labels for training data \n",
    "# output is 50,000 labels (50000 sentences for 10000 descriptions)\n",
    "train_y_vals = make_labels(N_DESCRIPTIONS)  # number of descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # build logistic regression model to map descriptions to photo label\n",
    "lr = LogisticRegression()\n",
    "lr.fit(train_x_vals, train_y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get word2vecs for all the test data by description\n",
    "test_x_vals = get_w2v_list_by_description(pre_pro_test)  # pass in preprocessed test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 500)\n"
     ]
    }
   ],
   "source": [
    "# print(np.array(test_x_vals).shape)  # for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict test y-vals photo labels on the training model\n",
    "estimated_train_indexes = lr.predict(test_x_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(estimated_train_indexes)  # for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using these estimated training indexes, pull up their image feature vectors\n",
    "\n",
    "estimates_features = []  # declare a list of dataframes\n",
    "\n",
    "for elem in estimated_train_indexes: \n",
    "    estimates_features.append(features_train.iloc[elem])  # retrieve feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, now that we mapped a test description to a predicted feature vector in the training data, we find the nearest 20 neighbors in the test data as our prediction.\n",
    "\n",
    "# kNN - for each test data, find the nearest 20 neighbors as our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_labels = list(features_test.index.values)  # grab the features from the test data\n",
    "knn = KNN(n_neighbors = 20)  # build knn model using k = 20 (required for submission)\n",
    "knn = knn.fit(features_test,test_labels)  # fit the features and test labels\n",
    "\n",
    "# calculate predictions and output into pandas dataframe for Kaggle contest format\n",
    "predictions = knn.kneighbors(estimates_features,return_distance=False)\n",
    "predictions = np.insert(predictions,0,list(range(N_TEST)),axis=1)\n",
    "predictions = pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Descritpion_ID</th>\n",
       "      <th>Top_20_Image_IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.txt</td>\n",
       "      <td>669.jpg 231.jpg 46.jpg 1992.jpg 1480.jpg 92.jp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.txt</td>\n",
       "      <td>589.jpg 1714.jpg 818.jpg 1753.jpg 1199.jpg 149...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.txt</td>\n",
       "      <td>1183.jpg 838.jpg 1471.jpg 634.jpg 42.jpg 598.j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.txt</td>\n",
       "      <td>1514.jpg 469.jpg 1765.jpg 26.jpg 235.jpg 1126....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.txt</td>\n",
       "      <td>305.jpg 1243.jpg 1980.jpg 870.jpg 105.jpg 1380...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.txt</td>\n",
       "      <td>846.jpg 1913.jpg 1118.jpg 413.jpg 1882.jpg 170...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.txt</td>\n",
       "      <td>1944.jpg 1855.jpg 1377.jpg 1940.jpg 897.jpg 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.txt</td>\n",
       "      <td>1904.jpg 364.jpg 20.jpg 1203.jpg 1400.jpg 1258...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.txt</td>\n",
       "      <td>1004.jpg 923.jpg 1261.jpg 1031.jpg 1896.jpg 71...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.txt</td>\n",
       "      <td>351.jpg 1321.jpg 1700.jpg 1145.jpg 249.jpg 600...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.txt</td>\n",
       "      <td>1471.jpg 42.jpg 634.jpg 1084.jpg 511.jpg 260.j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.txt</td>\n",
       "      <td>1254.jpg 51.jpg 1384.jpg 1446.jpg 1402.jpg 132...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.txt</td>\n",
       "      <td>1768.jpg 304.jpg 1948.jpg 1885.jpg 805.jpg 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.txt</td>\n",
       "      <td>1470.jpg 1944.jpg 1655.jpg 996.jpg 1311.jpg 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.txt</td>\n",
       "      <td>217.jpg 1311.jpg 262.jpg 996.jpg 897.jpg 1758....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.txt</td>\n",
       "      <td>1369.jpg 247.jpg 1839.jpg 198.jpg 1141.jpg 149...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.txt</td>\n",
       "      <td>68.jpg 1224.jpg 1082.jpg 1610.jpg 808.jpg 1243...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.txt</td>\n",
       "      <td>1487.jpg 600.jpg 561.jpg 523.jpg 496.jpg 607.j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.txt</td>\n",
       "      <td>764.jpg 1279.jpg 417.jpg 427.jpg 670.jpg 172.j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19.txt</td>\n",
       "      <td>275.jpg 933.jpg 1620.jpg 1301.jpg 1189.jpg 573...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20.txt</td>\n",
       "      <td>1847.jpg 1570.jpg 103.jpg 1283.jpg 1919.jpg 72...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21.txt</td>\n",
       "      <td>1661.jpg 1234.jpg 220.jpg 963.jpg 308.jpg 1026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22.txt</td>\n",
       "      <td>1138.jpg 1814.jpg 1754.jpg 1891.jpg 1524.jpg 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23.txt</td>\n",
       "      <td>1349.jpg 1422.jpg 733.jpg 1554.jpg 780.jpg 135...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24.txt</td>\n",
       "      <td>584.jpg 536.jpg 1062.jpg 1633.jpg 522.jpg 534....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25.txt</td>\n",
       "      <td>1215.jpg 471.jpg 960.jpg 1097.jpg 731.jpg 1778...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26.txt</td>\n",
       "      <td>1147.jpg 668.jpg 482.jpg 1141.jpg 1199.jpg 107...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27.txt</td>\n",
       "      <td>1018.jpg 1633.jpg 101.jpg 1042.jpg 947.jpg 637...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28.txt</td>\n",
       "      <td>587.jpg 1615.jpg 428.jpg 1184.jpg 1989.jpg 225...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29.txt</td>\n",
       "      <td>1789.jpg 1118.jpg 82.jpg 1191.jpg 1355.jpg 158...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>1970.txt</td>\n",
       "      <td>844.jpg 1611.jpg 420.jpg 982.jpg 1365.jpg 1013...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>1971.txt</td>\n",
       "      <td>1614.jpg 731.jpg 1306.jpg 805.jpg 1207.jpg 188...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>1972.txt</td>\n",
       "      <td>1169.jpg 385.jpg 1274.jpg 1545.jpg 1356.jpg 45...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>1973.txt</td>\n",
       "      <td>545.jpg 607.jpg 523.jpg 1211.jpg 117.jpg 1183....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>1974.txt</td>\n",
       "      <td>533.jpg 490.jpg 1939.jpg 424.jpg 596.jpg 889.j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>1975.txt</td>\n",
       "      <td>1888.jpg 1602.jpg 1206.jpg 1018.jpg 1913.jpg 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>1976.txt</td>\n",
       "      <td>1470.jpg 1944.jpg 1655.jpg 996.jpg 1311.jpg 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>1977.txt</td>\n",
       "      <td>1415.jpg 418.jpg 26.jpg 1009.jpg 1514.jpg 235....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>1978.txt</td>\n",
       "      <td>262.jpg 1151.jpg 1289.jpg 996.jpg 910.jpg 1940...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>1979.txt</td>\n",
       "      <td>1117.jpg 473.jpg 1384.jpg 1466.jpg 1782.jpg 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>1980.txt</td>\n",
       "      <td>474.jpg 618.jpg 1595.jpg 122.jpg 412.jpg 884.j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>1981.txt</td>\n",
       "      <td>733.jpg 39.jpg 1356.jpg 1545.jpg 780.jpg 759.j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>1982.txt</td>\n",
       "      <td>1890.jpg 1449.jpg 417.jpg 1932.jpg 857.jpg 149...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>1983.txt</td>\n",
       "      <td>1739.jpg 1853.jpg 1962.jpg 398.jpg 1164.jpg 74...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>1984.txt</td>\n",
       "      <td>1316.jpg 76.jpg 1643.jpg 1452.jpg 1636.jpg 182...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>1985.txt</td>\n",
       "      <td>153.jpg 1720.jpg 1553.jpg 1919.jpg 749.jpg 717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>1986.txt</td>\n",
       "      <td>1182.jpg 890.jpg 713.jpg 1159.jpg 1160.jpg 622...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>1987.txt</td>\n",
       "      <td>1685.jpg 143.jpg 1038.jpg 1157.jpg 1491.jpg 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>1988.txt</td>\n",
       "      <td>1813.jpg 544.jpg 151.jpg 1473.jpg 1621.jpg 171...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>1989.txt</td>\n",
       "      <td>385.jpg 1274.jpg 1019.jpg 1356.jpg 1964.jpg 73...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>1990.txt</td>\n",
       "      <td>1592.jpg 324.jpg 1256.jpg 524.jpg 858.jpg 1245...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>1991.txt</td>\n",
       "      <td>804.jpg 589.jpg 1054.jpg 355.jpg 997.jpg 326.j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>1992.txt</td>\n",
       "      <td>156.jpg 217.jpg 655.jpg 1444.jpg 1151.jpg 555....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>1993.txt</td>\n",
       "      <td>1938.jpg 659.jpg 991.jpg 1784.jpg 924.jpg 1281...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>1994.txt</td>\n",
       "      <td>1556.jpg 111.jpg 1792.jpg 1833.jpg 1864.jpg 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1995.txt</td>\n",
       "      <td>1242.jpg 838.jpg 511.jpg 883.jpg 1896.jpg 1183...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1996.txt</td>\n",
       "      <td>1853.jpg 399.jpg 180.jpg 1508.jpg 1605.jpg 938...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1997.txt</td>\n",
       "      <td>1945.jpg 1166.jpg 199.jpg 1026.jpg 1163.jpg 43...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>1998.txt</td>\n",
       "      <td>922.jpg 1298.jpg 1510.jpg 917.jpg 985.jpg 1399...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1999.txt</td>\n",
       "      <td>62.jpg 1868.jpg 1498.jpg 383.jpg 1588.jpg 473....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Descritpion_ID                                   Top_20_Image_IDs\n",
       "0             0.txt  669.jpg 231.jpg 46.jpg 1992.jpg 1480.jpg 92.jp...\n",
       "1             1.txt  589.jpg 1714.jpg 818.jpg 1753.jpg 1199.jpg 149...\n",
       "2             2.txt  1183.jpg 838.jpg 1471.jpg 634.jpg 42.jpg 598.j...\n",
       "3             3.txt  1514.jpg 469.jpg 1765.jpg 26.jpg 235.jpg 1126....\n",
       "4             4.txt  305.jpg 1243.jpg 1980.jpg 870.jpg 105.jpg 1380...\n",
       "5             5.txt  846.jpg 1913.jpg 1118.jpg 413.jpg 1882.jpg 170...\n",
       "6             6.txt  1944.jpg 1855.jpg 1377.jpg 1940.jpg 897.jpg 11...\n",
       "7             7.txt  1904.jpg 364.jpg 20.jpg 1203.jpg 1400.jpg 1258...\n",
       "8             8.txt  1004.jpg 923.jpg 1261.jpg 1031.jpg 1896.jpg 71...\n",
       "9             9.txt  351.jpg 1321.jpg 1700.jpg 1145.jpg 249.jpg 600...\n",
       "10           10.txt  1471.jpg 42.jpg 634.jpg 1084.jpg 511.jpg 260.j...\n",
       "11           11.txt  1254.jpg 51.jpg 1384.jpg 1446.jpg 1402.jpg 132...\n",
       "12           12.txt  1768.jpg 304.jpg 1948.jpg 1885.jpg 805.jpg 199...\n",
       "13           13.txt  1470.jpg 1944.jpg 1655.jpg 996.jpg 1311.jpg 12...\n",
       "14           14.txt  217.jpg 1311.jpg 262.jpg 996.jpg 897.jpg 1758....\n",
       "15           15.txt  1369.jpg 247.jpg 1839.jpg 198.jpg 1141.jpg 149...\n",
       "16           16.txt  68.jpg 1224.jpg 1082.jpg 1610.jpg 808.jpg 1243...\n",
       "17           17.txt  1487.jpg 600.jpg 561.jpg 523.jpg 496.jpg 607.j...\n",
       "18           18.txt  764.jpg 1279.jpg 417.jpg 427.jpg 670.jpg 172.j...\n",
       "19           19.txt  275.jpg 933.jpg 1620.jpg 1301.jpg 1189.jpg 573...\n",
       "20           20.txt  1847.jpg 1570.jpg 103.jpg 1283.jpg 1919.jpg 72...\n",
       "21           21.txt  1661.jpg 1234.jpg 220.jpg 963.jpg 308.jpg 1026...\n",
       "22           22.txt  1138.jpg 1814.jpg 1754.jpg 1891.jpg 1524.jpg 1...\n",
       "23           23.txt  1349.jpg 1422.jpg 733.jpg 1554.jpg 780.jpg 135...\n",
       "24           24.txt  584.jpg 536.jpg 1062.jpg 1633.jpg 522.jpg 534....\n",
       "25           25.txt  1215.jpg 471.jpg 960.jpg 1097.jpg 731.jpg 1778...\n",
       "26           26.txt  1147.jpg 668.jpg 482.jpg 1141.jpg 1199.jpg 107...\n",
       "27           27.txt  1018.jpg 1633.jpg 101.jpg 1042.jpg 947.jpg 637...\n",
       "28           28.txt  587.jpg 1615.jpg 428.jpg 1184.jpg 1989.jpg 225...\n",
       "29           29.txt  1789.jpg 1118.jpg 82.jpg 1191.jpg 1355.jpg 158...\n",
       "...             ...                                                ...\n",
       "1970       1970.txt  844.jpg 1611.jpg 420.jpg 982.jpg 1365.jpg 1013...\n",
       "1971       1971.txt  1614.jpg 731.jpg 1306.jpg 805.jpg 1207.jpg 188...\n",
       "1972       1972.txt  1169.jpg 385.jpg 1274.jpg 1545.jpg 1356.jpg 45...\n",
       "1973       1973.txt  545.jpg 607.jpg 523.jpg 1211.jpg 117.jpg 1183....\n",
       "1974       1974.txt  533.jpg 490.jpg 1939.jpg 424.jpg 596.jpg 889.j...\n",
       "1975       1975.txt  1888.jpg 1602.jpg 1206.jpg 1018.jpg 1913.jpg 5...\n",
       "1976       1976.txt  1470.jpg 1944.jpg 1655.jpg 996.jpg 1311.jpg 12...\n",
       "1977       1977.txt  1415.jpg 418.jpg 26.jpg 1009.jpg 1514.jpg 235....\n",
       "1978       1978.txt  262.jpg 1151.jpg 1289.jpg 996.jpg 910.jpg 1940...\n",
       "1979       1979.txt  1117.jpg 473.jpg 1384.jpg 1466.jpg 1782.jpg 11...\n",
       "1980       1980.txt  474.jpg 618.jpg 1595.jpg 122.jpg 412.jpg 884.j...\n",
       "1981       1981.txt  733.jpg 39.jpg 1356.jpg 1545.jpg 780.jpg 759.j...\n",
       "1982       1982.txt  1890.jpg 1449.jpg 417.jpg 1932.jpg 857.jpg 149...\n",
       "1983       1983.txt  1739.jpg 1853.jpg 1962.jpg 398.jpg 1164.jpg 74...\n",
       "1984       1984.txt  1316.jpg 76.jpg 1643.jpg 1452.jpg 1636.jpg 182...\n",
       "1985       1985.txt  153.jpg 1720.jpg 1553.jpg 1919.jpg 749.jpg 717...\n",
       "1986       1986.txt  1182.jpg 890.jpg 713.jpg 1159.jpg 1160.jpg 622...\n",
       "1987       1987.txt  1685.jpg 143.jpg 1038.jpg 1157.jpg 1491.jpg 19...\n",
       "1988       1988.txt  1813.jpg 544.jpg 151.jpg 1473.jpg 1621.jpg 171...\n",
       "1989       1989.txt  385.jpg 1274.jpg 1019.jpg 1356.jpg 1964.jpg 73...\n",
       "1990       1990.txt  1592.jpg 324.jpg 1256.jpg 524.jpg 858.jpg 1245...\n",
       "1991       1991.txt  804.jpg 589.jpg 1054.jpg 355.jpg 997.jpg 326.j...\n",
       "1992       1992.txt  156.jpg 217.jpg 655.jpg 1444.jpg 1151.jpg 555....\n",
       "1993       1993.txt  1938.jpg 659.jpg 991.jpg 1784.jpg 924.jpg 1281...\n",
       "1994       1994.txt  1556.jpg 111.jpg 1792.jpg 1833.jpg 1864.jpg 16...\n",
       "1995       1995.txt  1242.jpg 838.jpg 511.jpg 883.jpg 1896.jpg 1183...\n",
       "1996       1996.txt  1853.jpg 399.jpg 180.jpg 1508.jpg 1605.jpg 938...\n",
       "1997       1997.txt  1945.jpg 1166.jpg 199.jpg 1026.jpg 1163.jpg 43...\n",
       "1998       1998.txt  922.jpg 1298.jpg 1510.jpg 917.jpg 985.jpg 1399...\n",
       "1999       1999.txt  62.jpg 1868.jpg 1498.jpg 383.jpg 1588.jpg 473....\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save predictions to csv using our setup module we built\n",
    "setup.save_output(predictions,'eric-300pm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
